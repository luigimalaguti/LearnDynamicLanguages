{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Analisi lessicale\n",
    "\n",
    "Il primo e fondamentale ruolo dell'analisi lessicale è la tokenizzazione del codice sorgente di input. Ogni token è un elemento linguistico per la successiva fase di analisi sintattica.\n",
    "\n",
    "Lo scanner, colui che svolge l'analisi lessicale, esegue anche altre importanti attività:\n",
    "\n",
    "- Riconoscere e filtrare commenti, spazi e altri caratteri di separazione\n",
    "- Registrare la posizione di occorrenza nel file di input dei vari token. Questo permette in un secondo momento, qualora dovessero riscontrarsi errori, di emettere messaggi di diagnostica precisi\n",
    "- Procedere all'eventuale espansione delle macro\n",
    "\n",
    "Noi analizzeremo principalmente il procedimento di tokenizzazione.\n",
    "\n",
    "## Espressioni regolari pure\n",
    "\n",
    "Una espressione regolare è una scrittura che rappresenta un linguaggio di un dato alfabeto. La definizione di espressioni regolari pure procede per ricorrenza partendo proprio dall'alfabeto di riferimento.\n",
    "\n",
    "Le due seguenti definizioni costituiscono la base della costruzione ricorsiva:\n",
    "\n",
    "- Se $E$ e $F$ sono due espressioni regolari che denotano rispettivamente i linguaggi $L_E$ e $L_F$, allora la scrittura $EF$ è a sua volta una espressione regolare che denota il seguente linguaggio $$L_{EF} = \\left \\{ z : z = xy, x \\in L_E, y \\in L_F \\right \\}$$\n",
    "\n",
    "- Se $E$ ed $F$ sono espressioni regolari che denotano rispettivamente i linguaggi $L_E$ e $L_F$, allora la scrittura $E|F$ è anch'essa un'espressione regolare che denota il seguente linguaggio $$L_{E|F} = \\left \\{ z : z \\in L_E \\quad oppure \\quad z \\in L_F \\right \\}$$\n",
    "Il linguaggio dell'unione di due espressioni regolari $L_{E|F}$ è possibile scriverlo anche come $L_E|L_F$.\n",
    "\n",
    "Vediamo un primo esempio per comprendere il significato.  \n",
    "Consideriamo il seguente alfabeto $\\left \\{ 0, 1 \\right \\}$:\n",
    "\n",
    "- $\\textbf{00}$ è la concatenazione di due espressioni regolari uguali, ovvero $\\textbf{0}$, quindi il linguaggio corrispondente risulta $\\left \\{ 00 \\right \\}$\n",
    "- $\\textbf{0|1}$ è l'unione di due espressioni regolari diverse, ovvero $\\textbf{0}$ e $\\textbf{1}$, quindi il linguaggio corrispondente risulta $\\left \\{ 0, 1 \\right \\}$\n",
    "\n",
    "Anche gli operatori nelle espressioni regolari hanno ordini di precedenza. L'operatore di concatenazione ha la precedenza sull'operatore di unione. Inoltre, come nell'analisi matematica, è possibile utilizzare le parentesi tonde () per forzare, cambiare, l'ordine di precedenza.\n",
    "\n",
    "Facciamo un esempio:\n",
    "\n",
    "- La seguente espressione regolare $\\textbf{00}|\\textbf{1}$ può essere interpretato come $(\\textbf{00})|\\textbf{1}$ e risulta il seguente linguaggio $\\left \\{ 00, 1 \\right \\}$\n",
    "- La seguente espressione regolare $\\textbf{0}(\\textbf{0}|\\textbf{1})$ ha invece il seguente linguaggio $\\left \\{ 00, 01 \\right \\}$\n",
    "\n",
    "Completiamo la definizione con le seguenti proprietà:\n",
    "\n",
    "- Se $L$ è un linguaggio, la potenza n-esima di $L$ si definisce come la concatenazione tra $L$ e $L^{n-1}$\n",
    "$$L^n = LL^{n-1} = L^{n-1}L, n>0$$\n",
    "- La chiusura riflettiva e transitiva di un linguaggio $L$ si definisce nel modo seguente $$L^* = \\bigcup_{n=0}^{\\infty} L^n$$\n",
    "- Se $E$ è un'espressione regolare che denota il linguaggio $L_E$, allora $E^*$ è un'espressione regolare che denota il linguaggio $L_E^*$\n",
    "\n",
    "Vediamo un esempio per ognuna delle ultime propietà viste.\n",
    "\n",
    "- Se $L = \\left \\{ 1, 10 \\right \\}$, avremo che $$L^2 = \\left \\{ 11, 110, 101, 1010 \\right \\}$$ $$L^3 = \\left \\{ 111, 1110, 1101, 11010, 1011, 10110, 10101, 101010 \\right \\}$$\n",
    "- Se $L = \\left \\{ 1, 10 \\right \\}$, avremo che $$L^* = \\left \\{ \\varepsilon, L^1, L^2, L^3, ... \\right \\}$$ $$L^* = \\left \\{ \\varepsilon, 1, 10, 11, 110, 101, 1010, 111, 1110, 1101, 11010, 1011, 10110, 10101, 101010, ... \\right \\}$$\n",
    "\n",
    "Con $\\varepsilon$ intendiamo una stringa nulla.\n",
    "\n",
    "## Metacaratteri\n",
    "\n",
    "Definiamo metacaratteri quei simboli che non fanno parte del linguaggio ma che servono per specificare formalmente il linguaggio. Nel caso delle espressioni regolari su un alfabeto $A$, sono metacaratteri i simboli usati per le espressioni regolati. Sono metacaratteri anche i seguenti simboli $*$, $|$.\n",
    "\n",
    "Nel caso uno dei metacaratteri fosse anche un simbolo del linguaggio stesso, allora quest'ultimo deve essere preceduto da un ```\\```. Questo carattere viene chiamato ```escaped```.\n",
    "\n",
    "Python possiede molti metacaratteri, alcuni esempi sono i seguenti:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ciao\nMondo!\n"
     ]
    }
   ],
   "source": [
    "# \\n metacarattere new line\n",
    "string = \"Ciao\\nMondo!\"\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ciao\tMondo!\n"
     ]
    }
   ],
   "source": [
    "# \\t metacarattere tabulazione\n",
    "string = \"Ciao\\tMondo!\"\n",
    "print(string)"
   ]
  },
  {
   "source": [
    "Python quando incontra un \"\\\" non lo interpreta come carattere della stirnga, ma bensì come un segnale che ciò che dovrà essere interpretata è una sequenza di caratteri.\n",
    "\n",
    "In python esistono le cosidette ```raw string```, ovvero stringhe in cui ogni carattere è interpretato singolarmente."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ciao\\nMondo!\n"
     ]
    }
   ],
   "source": [
    "# raw string\n",
    "string = r\"Ciao\\nMondo!\"\n",
    "print(string)"
   ]
  },
  {
   "source": [
    "Le raw string giocano un ruolo importante nella specifica delle espressioni regolari.\n",
    "\n",
    "## Espressioni regolari estese\n",
    "\n",
    "Utilizzando solamente le proprietà delle espressioni regolari esposte precedentemente, ovvero per le espressioni regolari pure, si rinscontrerebbero limitazioni inaccettabili in applicazioni reali vere. Come ad esempio il volere costruire un linguaggio, usando le espressioni regolari, costituito dall'instero alfabeto ASCII a differenza del carattere A che non lo vogliamo. L'unico modo consiste nello scrivere una unione di 255 espressioni regolari, dove la 256-esima è il carattere A.\n",
    "\n",
    "Notiamo che l'utilizzo delle espressioni regolari pure è molto stringente. Detto ciò, si adottano delle ulteriori regole e convenzioni per rendere più agile l'uso delle espressioni regolari. Vediamole:\n",
    "\n",
    "- $E^+$ denota l'espressione regolare £EE^*£ $$0^+ = 00^* = \\left \\{ 0, 00, 000, ...\\right \\}$$\n",
    "- $E^n$ = $EE...E \\ n \\ volte$\n",
    "- $E^n_m = E^M|E^{m+1}|...|E^n$ denota un intervallo di potenze\n",
    "- $[c_1c_2...c_n]$ denota l'insieme di caratteri $\\left \\{ c_1, c_2, ..., c_n \\right \\}$\n",
    "- $[\\hat{} c_1c_2...c_n]$ denota l'insieme di caratteri dell'alfabeto esclusi $c_1, c_2, ..., c_n$\n",
    "- $.$ denota un qualsiasi carattere dell'alfabeto di riferimento\n",
    "\n",
    "Nella maggior parte di strumenti che utilizzano le espressioni regolari non si possono scrivere apici o pendici, di conseguenza si utilizzano le seguenti scritture:\n",
    "\n",
    "- $E\\left \\{ n \\right \\}$ per $E^n$\n",
    "- $E\\left \\{ m, n \\right \\}$ per $E^n_m$\n",
    "- $E\\left \\{ ,n \\right \\}$ per $E^n_0$\n",
    "- $E\\left \\{ m, \\right \\}$ per $E^{\\infty}_m$\n",
    "\n",
    "## Espressioni regolari in python\n",
    "\n",
    "Per utilizzare le espressioni regolari in python si utilizza il modulo ```re```."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "source": [
    "Il module re contiene diverse funzioni. La prima che vediamo è ```re.compile```. Questa funzione riceve in input una stringa che rappresenta la nostra espressione regolare e restituisce in output un oggetto di tipo pattern.\n",
    "\n",
    "L'oggetto pattern restituito ci permette di utilizzare diversi metodi di ricerca su un testo che deve essere specificato. Il primo metodo che utilizziamo è ```match```. Questo metodo riceve come parametro una stringa di testo sul quale applicare l'espressione regolare. Match restituisce un oggetto di tipo match nel caso di risultato positivo altrimenti restituisce None nel caso negativo. Il metodo match controlla se la stringa di testo da valutare incomicia con l'espressione regolare."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"In\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<re.Match object; span=(0, 2), match='In'>\n"
     ]
    }
   ],
   "source": [
    "print(pattern.match(\"Informatica\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pattern.match(\"informatica\"))"
   ]
  },
  {
   "source": [
    "Sull'oggetto di tipo match è possibile utilizzare tre metodi:\n",
    "\n",
    "- start(), inizio del matching dell'espressione regolare\n",
    "- end(), fine del matching dell'espressione regolare\n",
    "- group(), insieme dei caratteri che formano il match"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Match: <re.Match object; span=(0, 2), match='In'>\nStart:\t 0\nEnd:\t 2\nGroup:\t In\n"
     ]
    }
   ],
   "source": [
    "match = pattern.match(\"Informatica\")\n",
    "\n",
    "print(\"Match:\", match)\n",
    "\n",
    "print(\"Start:\\t\", match.start())\n",
    "print(\"End:\\t\", match.end())\n",
    "print(\"Group:\\t\", match.group())"
   ]
  },
  {
   "source": [
    "Un secondo metodo che possiamo utilizzare su un oggetto di tipo pattern è ```search```. È analogo al metodo match con la differenza che search cerca una corrispondenza in tutta la stringa di testo e non solo all'inizio di essa come il match. In caso di corrispondenza restituisce un oggetto match altrimenti None."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Match: <re.Match object; span=(0, 2), match='In'>\nStart:\t 0\nEnd:\t 2\nGroup:\t In\nSearch: <re.Match object; span=(0, 2), match='In'>\nStart:\t 0\nEnd:\t 2\nGroup:\t In\n"
     ]
    }
   ],
   "source": [
    "match = pattern.match(\"Ingegneria Informatica\")\n",
    "search = pattern.search(\"Ingegneria Informatica\")\n",
    "\n",
    "print(\"Match:\", match)\n",
    "\n",
    "print(\"Start:\\t\", match.start())\n",
    "print(\"End:\\t\", match.end())\n",
    "print(\"Group:\\t\", match.group())\n",
    "\n",
    "print(\"Search:\", search)\n",
    "\n",
    "print(\"Start:\\t\", search.start())\n",
    "print(\"End:\\t\", search.end())\n",
    "print(\"Group:\\t\", search.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Match: None\nSearch: <re.Match object; span=(11, 13), match='In'>\nStart:\t 11\nEnd:\t 13\nGroup:\t In\n"
     ]
    }
   ],
   "source": [
    "match = pattern.match(\"ingegneria Informatica\")\n",
    "search = pattern.search(\"ingegneria Informatica\")\n",
    "\n",
    "print(\"Match:\", match)\n",
    "\n",
    "if match:\n",
    "    print(\"Start:\\t\", match.start())\n",
    "    print(\"End:\\t\", match.end())\n",
    "    print(\"Group:\\t\", match.group())\n",
    "\n",
    "print(\"Search:\", search)\n",
    "\n",
    "print(\"Start:\\t\", search.start())\n",
    "print(\"End:\\t\", search.end())\n",
    "print(\"Group:\\t\", search.group())"
   ]
  },
  {
   "source": [
    "Come si nota dai due esempi sopra, il metodo search cerca si in tutta la stringa di testo una corrispondenza, ma non trova tutte le corrispondenze. Infatti appena trovata una, esso termina. Nel caso volessimo trovare tutte le corrispondenze nel testo, il metodo da utilizzare è un altro.\n",
    "\n",
    "Il metodo in questione ```findall```, è il terzo metodo che analizziamo. Esso, come accennato, è quel metodo che ci permette di trovare tutte le corrispondenze in tutta la stringa di testo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Findall: ['In', 'In']\n"
     ]
    }
   ],
   "source": [
    "findall = pattern.findall(\"Ingegneria Informatica\")\n",
    "\n",
    "print(\"Findall:\", findall)"
   ]
  },
  {
   "source": [
    "Findall ci permette sostanzialmente di effettuare molteplici search finchè non termina il testo. Inoltre, findall ci ritorna direttamente la corrispondenza e non un oggetto match.\n",
    "\n",
    "Finall è possibile implementarlo semplicemente con un search ed un while."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In at 0\nIn at 11\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "while match := pattern.search(\"Ingegneria Informatica\", index):\n",
    "    print(f\"{match.group()} at {match.start()}\")\n",
    "    index = match.end()"
   ]
  },
  {
   "source": [
    "L'ultimo metodo importante che utilizziamo su di un oggetto pattern è ```finditer```. Finditer è analoogo a findall con la sola differenza che restituisce un iterabile anzichè una lista."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finditer: <callable_iterator object at 0x114464ca0>\n<re.Match object; span=(0, 2), match='In'>\n<re.Match object; span=(11, 13), match='In'>\n"
     ]
    }
   ],
   "source": [
    "finditer = pattern.finditer(\"Ingegneria Informatica\")\n",
    "\n",
    "print(\"Finditer:\", finditer)\n",
    "\n",
    "for match in finditer:\n",
    "    print(match)"
   ]
  },
  {
   "source": [
    "Solitamente, nella pratica, non si usa mai compilare il pattern separatamente, ma piuttosto specificare l'espressione regolare ed il testo nello stesso metodo.\n",
    "\n",
    "Proviamo a concludere le espressioni regolari con un esempio un pò più interessante.\n",
    "\n",
    "Vogliamo trovare tutti i match dell'espressione regolare ```[CG]{3, }``` in una stringa che rappresenta una porzione di DNA umano.\n",
    "\n",
    "La stringa in input è la seguente:\n",
    "\n",
    "> \"GAGGATTAGGTCTGGGACACGGAGAGGGTGTCCCTTCCTCATCCCCAGGT\"\n",
    "\n",
    "I risultati che attendiamo sono i seguenti:\n",
    "\n",
    "- GGG alla posizione 13\n",
    "- CGG alla posizione 19\n",
    "- GGG alla posizione 25\n",
    "- CCC alla posizione 31\n",
    "- CCCC alla posizione 42"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GGG at 13\nCGG at 19\nGGG at 25\nCCC at 31\nCCCC at 42\n"
     ]
    }
   ],
   "source": [
    "dna = \"GAGGATTAGGTCTGGGACACGGAGAGGGTGTCCCTTCCTCATCCCCAGGT\"\n",
    "\n",
    "matches = re.finditer(r\"[CG]{3,}\", dna)\n",
    "\n",
    "for match in matches:\n",
    "    print(f\"{match.group()} at {match.start()}\")"
   ]
  },
  {
   "source": [
    "Una cosa molto importante quando si specificano le espressioni regolari è il come le si scrivono. Basta aggiungere uno spazio dopo la virgola dell'espressione regolare che il risultato diventa None."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna = \"GAGGATTAGGTCTGGGACACGGAGAGGGTGTCCCTTCCTCATCCCCAGGT\"\n",
    "\n",
    "matches = re.finditer(r\"[CG]{3, }\", dna)\n",
    "\n",
    "for match in matches:\n",
    "    print(f\"{match.group()} at {match.start()}\")"
   ]
  },
  {
   "source": [
    "## Generatori di scanner\n",
    "\n",
    "Un generatore di scanner è un software che viene istruito sulle espressioni regolari da riconoscere che restituisce un programma in grado di riconoscere effettivamente i token in una stringa di input.\n",
    "\n",
    "Il più famoso stumento di generatore di scanner è ```Lex```, il quale produce in output un codice C compatibile. In python il generatore più comune è ```PLY```.  \n",
    "Lex lo guarderemo velocemente, mentre PLY lo utilizzeremo più intensamente.\n",
    "\n",
    "La struttura logica di Lex si presenta in questo modo.\n",
    "\n",
    "![Image 5](images/image-5.png)\n",
    "\n",
    "Un generico programma Lex contiene tre sezioni separate da una riga contenente due soli ```%%```.\n",
    "\n",
    "```\n",
    "Dichiarazioni\n",
    "%%\n",
    "Regole di traduzione\n",
    "%%\n",
    "Funzioni ausiliarie\n",
    "```\n",
    "\n",
    "- Dichiarazioni\n",
    "\n",
    "    La sezione dichiarazioni contiene definizioni di costanti e variabili. In più, è la sezione nel quale si possono dare nomi alle espressioni regolari da potere utilizzare nelle sezioni successive.\n",
    "\n",
    "- Reegole di traduzione\n",
    "\n",
    "    La sezione più importante, dove viene definita la logica del programma. Questa sezione contiene le descrizioni dei token da riconoscere e le corrispondenti azioni che devono essere eseguite dallo scanner.\n",
    "\n",
    "- Funzioni ausiliarie\n",
    "\n",
    "    Questa sezione permette di definire funzioni ausiliarie nel caso di bisogno. Nel caso in cui lo scanner non è utilizzato come routine del parser o di altri programmi, la sezione funzioni ausiliarie può contenere la funzione main del programma. In questo caso, all'interno del main dovrà essere presente la chiamata allo scanner.\n",
    "\n",
    "Il seguente codice è il programma equivalente al comando ```wc``` di UNIX.\n",
    "\n",
    "\n",
    "```l\n",
    "%{\n",
    "    unsigned charCount = 0;\n",
    "    unsigned wordCount = 0;\n",
    "    unsigned lineCount = 0;\n",
    "%}\n",
    "\n",
    "%option noyywrap\n",
    "\n",
    "word [^ \\t\\n]+\n",
    "eol \\n\n",
    "\n",
    "%%\n",
    "\n",
    "{word}  {\n",
    "    wordCount++;\n",
    "    charCount += yyleng;\n",
    "}\n",
    "\n",
    "{eol}   {\n",
    "    charCount++;\n",
    "    lineCount++;\n",
    "}\n",
    "\n",
    ".       {\n",
    "    charCount++;\n",
    "}\n",
    "\n",
    "%%\n",
    "\n",
    "int main()\n",
    "{\n",
    "    yylex();\n",
    "    printf(\"%i %i %i\\n\", lineCount, wordCount, charCount);\n",
    "}\n",
    "```\n",
    "\n",
    "Proviamo ad utilizzarlo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 80\ndrwxr-xr-x@  7 luigimalaguti  staff   224B Dec 10 18:07 \u001b[1m\u001b[31m.\u001b[m\u001b[m/\ndrwxr-xr-x@ 10 luigimalaguti  staff   320B Dec 10 18:07 \u001b[1m\u001b[31m..\u001b[m\u001b[m/\n-rw-r--r--   1 luigimalaguti  staff    11K Dec  8 18:00 1_Compilatori_e_interpreti.ipynb\n-rw-r--r--   1 luigimalaguti  staff    20K Dec 10 17:48 2_Analisi_lessicale.ipynb\n-rw-r--r--   1 luigimalaguti  staff   1.0K Dec  8 18:04 README.md\ndrwxr-xr-x@  5 luigimalaguti  staff   160B Dec 10 18:07 \u001b[1m\u001b[31mcodes\u001b[m\u001b[m/\ndrwxr-xr-x@  7 luigimalaguti  staff   224B Dec 10 17:31 \u001b[1m\u001b[31mimages\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/luigimalaguti/Desktop/Designs/LearnDynamicLanguages/2_Architettura/codes/scanner\n"
     ]
    }
   ],
   "source": [
    "cd codes/scanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 16\ndrwxr-xr-x@ 4 luigimalaguti  staff   128B Dec 14 10:09 \u001b[1m\u001b[31m.\u001b[m\u001b[m/\ndrwxr-xr-x@ 5 luigimalaguti  staff   160B Dec 14 10:10 \u001b[1m\u001b[31m..\u001b[m\u001b[m/\n-rw-r--r--  1 luigimalaguti  staff   1.5K Dec 14 10:09 scanner.py\n-rw-r--r--  1 luigimalaguti  staff   358B Dec 14 10:08 wc.l\n"
     ]
    }
   ],
   "source": [
    "ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "!flex wc.l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 96\ndrwxr-xr-x@ 4 luigimalaguti  staff   128B Dec 10 18:10 \u001b[1m\u001b[31m.\u001b[m\u001b[m/\ndrwxr-xr-x@ 7 luigimalaguti  staff   224B Dec 10 18:10 \u001b[1m\u001b[31m..\u001b[m\u001b[m/\n-rw-r--r--  1 luigimalaguti  staff    43K Dec 10 18:10 lex.yy.c\n-rw-r--r--  1 luigimalaguti  staff   358B Dec 10 18:08 wc.l\n"
     ]
    }
   ],
   "source": [
    "ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc lex.yy.c -o wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 200\ndrwxr-xr-x@ 5 luigimalaguti  staff   160B Dec 10 18:11 \u001b[1m\u001b[31m.\u001b[m\u001b[m/\ndrwxr-xr-x@ 7 luigimalaguti  staff   224B Dec 10 18:11 \u001b[1m\u001b[31m..\u001b[m\u001b[m/\n-rw-r--r--  1 luigimalaguti  staff    43K Dec 10 18:10 lex.yy.c\n-rwxr-xr-x  1 luigimalaguti  staff    51K Dec 10 18:10 \u001b[35mwc\u001b[m\u001b[m*\n-rw-r--r--  1 luigimalaguti  staff   358B Dec 10 18:08 wc.l\n"
     ]
    }
   ],
   "source": [
    "ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1758 6161 44173\n"
     ]
    }
   ],
   "source": [
    "!./wc < lex.yy.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    1758    6161   44173 lex.yy.c\n"
     ]
    }
   ],
   "source": [
    "!wc lex.yy.c"
   ]
  },
  {
   "source": [
    "Ricordiamoci che IPython ha la possibilità di interpretare anche i comandi bash e quindi di compilare ed eseguire il nostro programma.\n",
    "\n",
    "Vediamo che come prima cosa abbiamo utilizzato il comanso ```flex```. Questo comando è il generatore di scanner. Passiamo ad esso il nostro codice per il comando wc e il generatore ci ritornerà un programma in grado di riconoscere le espressioni regolari specificate in esso.\n",
    "\n",
    "Flex restituisce un programma C come abbiamo già detto. Infatti se utilizziamo il comando ```gcc``` per la compilazione di esso, otterremo un eseguibile in grado di contare le linee, parole e caratteri di un file in input.\n",
    "\n",
    "Per passare un file al nostro comando wc abbiamo bisogno della ridirezione di input, il carattere ```<```.\n",
    "\n",
    "Infine c'è un confronto tra l'esecuzione del nostro comando wc con il comando wc implementato di sistema in UNIX.\n",
    "\n",
    "## Generazione mediante ply.lex\n",
    "\n",
    "Come detto precedentemente, approfondiamo il generatore di scanner ply.\n",
    "\n",
    "Ply è un modulo python che ci permette di descrivere come vogliamo che il generatore ci creii il nostro scanner.\n",
    "\n",
    "Ovviamente la costruzione del nostro scanner è analoga alla costruzione appena vista con Lex in C, ma cambiano i metodi con coi comunichiamo al generatore."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from ply import lex"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "source": [
    "La prima cosa che dobbiamo indicare al generatore sono tutti i token.\n",
    "\n",
    "La specifica dei token avviene in due modi differenti. Il primo consiste nella specifica dei token riservati, come ad esempio le istruzioni if, for, e altre del linguaggio python. I token riservati vengono specificati come un dizionario chiavi valori, dove la chiave corrisponde all'espressione regolare da associare al valore, ovvero il token."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserverd = {\n",
    "    'if': 'IF',\n",
    "    'else': 'ELSE',\n",
    "    'elif': 'ELIF',\n",
    "    'while': 'WHILE',\n",
    "    'def': 'DEF',\n",
    "    'return': 'RETURN',\n",
    "}"
   ]
  },
  {
   "source": [
    "Il secondo modo per specificare i token consiste in una lista contenenti tutti i token che ci interessano"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\n",
    "    'ID', 'NUMBER', 'COLON',\n",
    "    'COMMA', 'PLUS', 'MINUS', \n",
    "    'MUL', 'DIV', 'AND',\n",
    "    'OR', 'NOT', 'LPAR',\n",
    "    'RPAR', 'SEP', 'EQ',\n",
    "    'NEQ', 'LT', 'GT',\n",
    "    'LE', 'GE', 'ASSIGN',\n",
    "    'PRINT', 'PI'\n",
    "] + list(reserverd.values())"
   ]
  },
  {
   "source": [
    "e successivamente andiamo a specificare quali token rispondono a quali espressioni regolari. Ogni token può essere chiamato in python con la seguente scrittura ```t_TOKEN```. Questo ci permette di assegnare ad ogni token la rispettiva espressione regolare."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_COLON = r':'\n",
    "t_COMMA = r','\n",
    "t_PLUS  = r'\\+'\n",
    "t_MINUS = r'-'\n",
    "t_MUL   = r'\\*'\n",
    "t_DIV   = r'/'\n",
    "t_LPAR  = r'\\('\n",
    "t_RPAR  = r'\\)'\n",
    "t_ASSIGN = r'='\n",
    "t_SEP = r';'\n",
    "t_LT = r'<'\n",
    "t_GT = r'>'\n",
    "t_LE = r'<='\n",
    "t_GE = r'>='\n",
    "t_EQ = r'=='\n",
    "t_NEQ = r'!='\n",
    "t_AND = r'and\\b'\n",
    "t_OR = r'or\\b'\n",
    "t_NOT = r'not\\b'\n",
    "t_PI = r'pi\\b'\n",
    "t_PRINT = r'print\\b'"
   ]
  },
  {
   "source": [
    "Possono esserci però alcuni token nella nostra lista che hanno bisogno di eseguire determinati compiti dopo il riconoscimento. Per definire le espressioni regolari di questi token e le relative azioni, bisogna definire funzioni con la stessa convensione utilizzata dalle variabili, ovvero essere chiamate con il nome del token preceduto da t_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_ID(t):\n",
    "    r'[a-zA-Z_][a-zA-Z_0-9]*'\n",
    "    t.type = reserved.get(t.value,'ID')\n",
    "    return t\n",
    "\n",
    "def t_NUMBER(t):\n",
    "    r'\\d*\\.\\d+|\\d+'\n",
    "    v = float(t.value)\n",
    "    if v == int(v):\n",
    "        t.value = int(v)    \n",
    "    else:\n",
    "        t.value = v\n",
    "    return t"
   ]
  },
  {
   "source": [
    "Notiamo come l'espressione regolare nelle funzioni viene passata grazie a quella che è la docstring.\n",
    "\n",
    "Inoltre passiamo al generatore informazioni su token speciali."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)\n",
    "\n",
    "t_ignore  = ' \\t'\n",
    "\n",
    "def t_error(t):\n",
    "     print(\"Illegal character '%s'\" % t.value[0])\n",
    "     t.lexer.skip(1)"
   ]
  },
  {
   "source": [
    "Abbiamo completato il nostro scanner. Quello che ci rimane da fare è creare una variabile ```lexer = lex.lex()``` per poi passargli dell'input da analizzare.\n",
    "\n",
    "Il modulo ply da problemi di esecuzione proprio di quest'ultimo comando, quindi dal notebook corrente non riusciamo a vederlo, ma nella cartella ```codes``` è presente il file completo con anche il main di esecuzione. \n",
    "\n",
    "Per eseguire lo scanner di esempio basta digitare da terminale\n",
    "\n",
    "```python\n",
    "python scanner.py\n",
    "```\n",
    "\n",
    "Attenzione, bisogna prima scaricare il modulo ply con pip."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}